{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "from prompts.dataset_generation import general_instruction_01 as general_instruction\n",
    "from typing import List, Dict\n",
    "\n",
    "load_dotenv()  \n",
    "OPENAI_API_KEY  = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the JSON file\n",
    "def read_json(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: the file {file_path} was not found.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding the JSON: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Generation\n",
    "\n",
    "In this section, the datasets are actually generated to evaluate the email classification prompt. Specifically, the following datasets are created:\n",
    "\n",
    "- **General Dataset**  \n",
    "- **Edge Cases**\n",
    "- **Language and Cultural Diversity**\n",
    "- **Special Service Requests (SSR) Emphasis**\n",
    "- **High Complexity**\n",
    "- **Sentiment Variations**\n",
    "- **Diverse Writing Styles and Formats**\n",
    "- **Tool Requirement Variations**  \n",
    "- **Customer Status Unknown**\n",
    "- **Urgency and Priority Levels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing prompt: General Dataset\n",
      "Saving data to file: ./datasets/General_Dataset.csv\n",
      "Finished processing prompt: General Dataset\n",
      "\n",
      "Processing prompt: Edge Cases\n",
      "Saving data to file: ./datasets/Edge_Cases.csv\n",
      "Finished processing prompt: Edge Cases\n",
      "\n",
      "Processing prompt: Language and Cultural Diversity\n",
      "Saving data to file: ./datasets/Language_and_Cultural_Diversity.csv\n",
      "Finished processing prompt: Language and Cultural Diversity\n",
      "\n",
      "Processing prompt: Special Service Requests (SSR) Emphasis\n",
      "Saving data to file: ./datasets/Special_Service_Requests_(SSR)_Emphasis.csv\n",
      "Finished processing prompt: Special Service Requests (SSR) Emphasis\n",
      "\n",
      "Processing prompt: High Complexity\n",
      "Saving data to file: ./datasets/High_Complexity.csv\n",
      "Finished processing prompt: High Complexity\n",
      "\n",
      "Processing prompt: Sentiment Variations\n",
      "Saving data to file: ./datasets/Sentiment_Variations.csv\n",
      "Finished processing prompt: Sentiment Variations\n",
      "\n",
      "Processing prompt: Diverse Writing Styles and Formats\n",
      "Saving data to file: ./datasets/Diverse_Writing_Styles_and_Formats.csv\n",
      "Finished processing prompt: Diverse Writing Styles and Formats\n",
      "\n",
      "Processing prompt: Tool Requirement Variations\n",
      "Saving data to file: ./datasets/Tool_Requirement_Variations.csv\n",
      "Finished processing prompt: Tool Requirement Variations\n",
      "\n",
      "Processing prompt: Customer Status Unknown\n",
      "Saving data to file: ./datasets/Customer_Status_Unknown.csv\n",
      "Finished processing prompt: Customer Status Unknown\n",
      "\n",
      "Processing prompt: Urgency and Priority Levels\n",
      "Saving data to file: ./datasets/Urgency_and_Priority_Levels.csv\n",
      "Finished processing prompt: Urgency and Priority Levels\n",
      "Email dataset generation complete.\n"
     ]
    }
   ],
   "source": [
    "# CSV Fieldnames\n",
    "fieldnames = [\n",
    "    'subject',\n",
    "    'sender',\n",
    "    'recipients',\n",
    "    'body'\n",
    "]\n",
    "\n",
    "# Reading json file with prompts for dataset generator\n",
    "file_path = './config/prompt_configs.json'  \n",
    "prompts = read_json(file_path)\n",
    "\n",
    "# Lock for managing concurrent writes to the CSV file\n",
    "lock = threading.Lock()\n",
    "\n",
    "def create_enhanced_prompt(general_instructions: str, prompt_info: Dict, batch_size: int, fieldnames: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Creates an enhanced prompt combining general guidelines with specific scenario instructions.\n",
    "    \"\"\"\n",
    "    scenario_name = prompt_info[\"name\"]\n",
    "    scenario_instructions = prompt_info[\"instructions\"]\n",
    "    \n",
    "    system_content = f\"\"\"\n",
    "    {general_instructions}\n",
    "\n",
    "    SPECIFIC SCENARIO FOCUS: {scenario_name}\n",
    "    {scenario_instructions}\n",
    "    \"\"\"\n",
    "\n",
    "    user_content = f\"\"\"\n",
    "    Generate {batch_size} unique emails **one per each purpose categories** following these strict formatting guidelines:\n",
    "\n",
    "    1. Use \";\" as the separator between fields\n",
    "    2. Enclose all field values in double quotes (e.g., \"value1\";\"value2\";\"value3\")\n",
    "    3. Include only the raw CSV content with these fields: {'; '.join(fieldnames)}\n",
    "    4. No explanatory text or formatting markers\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "    }\n",
    "\n",
    "# Function to generate a batch of emails\n",
    "def generate_email_batch(prompt_info, batch_size, fieldnames):\n",
    "    \n",
    "    enhanced_prompt = create_enhanced_prompt(\n",
    "            general_instructions=general_instruction,\n",
    "            prompt_info=prompt_info,\n",
    "            batch_size=batch_size,\n",
    "            fieldnames=fieldnames\n",
    "        )\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Make API call\n",
    "        response = client.chat.completions.create(\n",
    "            model='gpt-4o', \n",
    "            messages=enhanced_prompt[\"messages\"],\n",
    "            temperature=enhanced_prompt[\"temperature\"],\n",
    "        )\n",
    "\n",
    "        # Extract the AI's response\n",
    "        assistant_reply = response.choices[0].message.content\n",
    "        lines = assistant_reply.strip().split('\\n')\n",
    "\n",
    "        # Filter unnecessary lines and process data\n",
    "        data_lines = [\n",
    "            line for line in lines \n",
    "            if line.strip() and not (line.startswith(\"```\") or line.startswith(\"```csv\") or line.startswith('\"subject\"'))\n",
    "        ]\n",
    "        reader = csv.DictReader(data_lines, fieldnames=fieldnames, delimiter=';')\n",
    "        return list(reader)  # Return a list of rows as dictionaries\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch generation: {e}\")\n",
    "        return []  # Return an empty list in case of an error\n",
    "\n",
    "# Function to process a single prompt\n",
    "def process_prompt(prompt_info, total_emails, output_file):\n",
    "    emails_generated = 0\n",
    "    batch_size = 12  # Batch size\n",
    "\n",
    "    # Create a filename for each prompt based on its name\n",
    "    sanitized_name = prompt_info['name'].replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    csv_filename = f\"./datasets/{sanitized_name}.csv\"\n",
    "\n",
    "    print(f\"\\nProcessing prompt: {prompt_info['name']}\")\n",
    "    print(f\"Saving data to file: {csv_filename}\")\n",
    "\n",
    "    # Open a new CSV file for each prompt\n",
    "    with open(csv_filename, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames, delimiter=';', quoting=csv.QUOTE_ALL)\n",
    "        writer.writeheader()\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=1000) as executor:  # Use up to 5 threads for parallelism\n",
    "            futures = []\n",
    "\n",
    "            # Schedule batches of email generation\n",
    "            while emails_generated < total_emails:\n",
    "                batch_to_generate = min(batch_size, total_emails - emails_generated)\n",
    "                futures.append(\n",
    "                    executor.submit(generate_email_batch, prompt_info, batch_to_generate, fieldnames)\n",
    "                )\n",
    "                emails_generated += batch_to_generate\n",
    "\n",
    "            # Write results to the CSV file\n",
    "            for future in futures:\n",
    "                rows = future.result()  # Collect results from the thread\n",
    "                with lock:  # Ensure only one thread writes at a time\n",
    "                    for row in rows:\n",
    "                        # Validate fields before writing\n",
    "                        if not row or any(key not in fieldnames for key in row.keys()):\n",
    "                            print(f\"Invalid row detected and skipped: {row}\")\n",
    "                            continue  # Skip invalid rows\n",
    "                        try:\n",
    "                            writer.writerow(row)\n",
    "                        except ValueError as e:\n",
    "                            print(f\"Error writing row: {row}, Error: {e}\")\n",
    "\n",
    "    print(f\"Finished processing prompt: {prompt_info['name']}\")\n",
    "\n",
    "\n",
    "# Process each prompt\n",
    "for prompt_info in prompts.get(\"prompts\", []):\n",
    "    process_prompt(prompt_info, prompt_info['emails_to_generate'], fieldnames)\n",
    "\n",
    "print(\"Email dataset generation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the Generated Datasets\n",
    "\n",
    "In this section, the previously created datasets are combined into a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: ./datasets\\Customer_Status_Unknown.csv\n",
      "Reading file: ./datasets\\Diverse_Writing_Styles_and_Formats.csv\n",
      "Reading file: ./datasets\\Edge_Cases.csv\n",
      "Reading file: ./datasets\\General_Dataset.csv\n",
      "Reading file: ./datasets\\High_Complexity.csv\n",
      "Reading file: ./datasets\\Language_and_Cultural_Diversity.csv\n",
      "Reading file: ./datasets\\Sentiment_Variations.csv\n",
      "Reading file: ./datasets\\Special_Service_Requests_(SSR)_Emphasis.csv\n",
      "Reading file: ./datasets\\Tool_Requirement_Variations.csv\n",
      "Reading file: ./datasets\\Urgency_and_Priority_Levels.csv\n",
      "Combined dataset saved to ./datasets/combined_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Path to the directory containing the CSV files\n",
    "directory = './datasets'\n",
    "\n",
    "# Name of the output file\n",
    "output_file = './datasets/combined_dataset.csv'\n",
    "\n",
    "# List to store the DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Read each CSV file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        print(f\"Reading file: {file_path}\")\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path, delimiter=';')\n",
    "        \n",
    "        # Add the DataFrame to the list\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Remove duplicate headers in the content (if any)\n",
    "combined_df = combined_df[~combined_df['subject'].str.contains('subject', na=False)]\n",
    "\n",
    "# Save the result into a single CSV file\n",
    "combined_df.to_csv(output_file, index=False, sep=';')\n",
    "\n",
    "print(f\"Combined dataset saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
